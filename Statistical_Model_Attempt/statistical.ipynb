{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3259: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from spacy.cli import download\n",
    "\n",
    "download(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003863573835380988\n"
     ]
    }
   ],
   "source": [
    "f = open(\"bigram_stats.txt\", encoding=\"utf-8\", errors=\"ignore\")\n",
    "lines = f.readlines()\n",
    "\n",
    "total = 0 \n",
    "bigram_count = dict()\n",
    "for line in lines:\n",
    "    key, value = line.split(\") \")\n",
    "    key = key[1:]\n",
    "    key = key.replace(\"'\", \"\")\n",
    "    first, second = key.split(\", \")\n",
    "    \n",
    "    bigram_count[(first, second)] = int(value)\n",
    "    total += int(value)\n",
    "\n",
    "for key, value in bigram_count.items():\n",
    "    bigram_count[key] /= total\n",
    "    \n",
    "print(bigram_count[(',', 'I')])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"output.txt\", encoding=\"utf-8\", errors=\"ignore\")\n",
    "lines = f.readlines()\n",
    "training = []\n",
    "correct = []\n",
    "for i in range(len(lines)):\n",
    "    if i == 6000:\n",
    "        break\n",
    "    if i%4 == 2 or i%4 == 3:\n",
    "        continue\n",
    "    if i%4 == 0:\n",
    "        training.append(lines[i].strip())\n",
    "    else:\n",
    "        correct.append(lines[i].strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('null', (5, 'the artificial lake'), [6, 0, 1, 6, 0, 0, 0]), ('null', (11, 'the city which is surrounded by a park'), [1, 6, -1, -1, 0, 0, 0])]\n"
     ]
    }
   ],
   "source": [
    "POS_dict = {\"ADJ\": 0, \"ADP\": 1, \"ADV\": 2, \"AUX\": 3, \"CONJ\": 4, \"CCONJ\": 5, \"DET\": 6, \"INTJ\": 7,\n",
    "            \"NOUN\": 8, \"NUM\": 9, \"PART\": 10, \"PRON\": 11, \"PROPN\": 12, \"SCONJ\": 13, \"SYM\": 14, \n",
    "            \"VERB\": 15, \"X\": 16, \"SPACE\": 17, \"PUNCT\": 18}\n",
    "type_list = [\"ADP\", \"NOUN\"]\n",
    "def get_pps(doc, _type, check_prev, test=False):\n",
    "    \"Function to get PPs from a parsed document.\"\n",
    "    pps = []\n",
    "    next = 0\n",
    "    for i in range(len(doc)):\n",
    "        if next != 0:\n",
    "            next -= 1\n",
    "            continue\n",
    "        # Try this with other parts of speech for different subtrees.\n",
    "        if doc[i].pos_ == _type:\n",
    "            subtree = list(doc[i].subtree)\n",
    "            pp = ' '\n",
    "            ADP_list = []\n",
    "            plural = 0\n",
    "            next = len(subtree)\n",
    "\n",
    "            word = \"\"\n",
    "            if i != 0:\n",
    "                if doc[i-1] in check_prev:\n",
    "                    word = doc[i-1]\n",
    "            if doc[i] in check_prev:\n",
    "                word = doc[i]\n",
    "            else:\n",
    "                word = \"null\"\n",
    "\n",
    "\n",
    "            if doc[i].tag_ == \"NNS\":\n",
    "                plural = 1\n",
    "            for j in range(len(subtree)):\n",
    "                ADP_list.append(subtree[j].orth_)\n",
    "            pp = ' '.join(ADP_list)\n",
    "            input_feature = []\n",
    "\n",
    "            for j in range(2, 0, -1):\n",
    "                if i - j < 0:\n",
    "                    input_feature.append(-1)\n",
    "                else:\n",
    "                    input_feature.append(POS_dict[doc[i - j].pos_])\n",
    "\n",
    "            for j in range(1, 3):\n",
    "                if i + j + next >= len(doc):\n",
    "                    input_feature.append(-1)\n",
    "                else:\n",
    "                    input_feature.append(POS_dict[doc[i + j + next].pos_])\n",
    "            \n",
    "            if plural == 1:\n",
    "                input_feature.append(1)\n",
    "            else:\n",
    "                input_feature.append(0)\n",
    "\n",
    "            if i - 1 < 0:\n",
    "                input_feature.append(bigram_count.get(('<start>', doc[i]), 0))\n",
    "            else:\n",
    "                input_feature.append(bigram_count.get((doc[i-1], doc[i]), 0))\n",
    "            \n",
    "            if i + 1 >= len(doc):\n",
    "                input_feature.append(bigram_count.get((doc[i], '<end>'), 0))\n",
    "            else:\n",
    "                input_feature.append(bigram_count.get((doc[i], doc[i + 1]), 0))\n",
    "                \n",
    "            pps.append((word, (i,pp), input_feature))\n",
    "    return pps\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "ex = \"I recommend visiting the artificial lake in the certer of the city which is surrounded by a park .\"\n",
    "doc = nlp(ex)\n",
    "\n",
    "print(get_pps(doc, type_list[1], {\"a\": 0, \"the\": 1, \"null\": 2, \"an\":0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def input_feature_transform(input_feature):\n",
    "    a = np.array(input_feature)\n",
    "    return a.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = [\"a\", \"the\", \"null\"]\n",
    "prep = [\"about\", \"along\", \"among\", \"around\", \"as\", \"at\", \"beside\", \"besides\", \"between\", \"by\", \"down\", \"during\",\n",
    "\"except\", \"for\", \"from\", \"in\", \"inside\", \"into\", \"of\", \"off\", \"on\", \"onto\", \"outside\", \"over\", \n",
    "\"through\", \"to\", \"toward\", \"towards\", \"under\", \"underneath\", \"until\", \"up\", \"upon\", \"with\", \"within\", \"without\"]\n",
    "\n",
    "article_dict = {\"a\": 0, \"the\": 1, \"null\": 2}\n",
    "prep_dict = {}\n",
    "for i in range(len(prep)):\n",
    "    prep_dict[prep[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 3101)\n",
      "(3101, 36)\n"
     ]
    }
   ],
   "source": [
    "def generate_input(dict_input, size):\n",
    "    X_article = []\n",
    "    Y_article = []\n",
    "    for i in range(size):\n",
    "        Y_article.append([])\n",
    "    for i in correct:\n",
    "        extract = get_pps(nlp(i), type_list[1], dict_input)\n",
    "        for j in extract:\n",
    "            X_article.append(j[2])\n",
    "            word = j[0]\n",
    "            index = dict_input.get(word, -1)\n",
    "            for k in Y_article:\n",
    "                if k == index:\n",
    "                    k.append(1)\n",
    "                else:\n",
    "                    k.append(-1)\n",
    "    X_input = input_feature_transform(X_article)\n",
    "    Y_input = input_feature_transform(Y_article)\n",
    "    return X_input, Y_input\n",
    "\n",
    "X_input, Y_input = generate_input(prep_dict, len(prep_dict))\n",
    "\n",
    "print(X_input.shape)\n",
    "print(Y_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99358788 0.83471854 0.21820746 0.74632216 0.14668985 0.44692038\n",
      "  0.70193668]]\n",
      "[6.67331466e-05 5.34698884e-05 4.11029078e-05 2.64650811e-05\n",
      " 5.34067438e-05 7.22289334e-06 4.90728553e-05]\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "class linear_classifier:\n",
    "    def __init__(self, X, Y, name) -> None:\n",
    "        self.u = np.random.rand(7,1)\n",
    "        self.lamb = 1e-4\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.name = name\n",
    "    def predict(self, X):\n",
    "        # self.X = input[0:]\n",
    "        # self.Y = input[1:]\n",
    "        compute = self.u.T @ X\n",
    "        return compute\n",
    "        \n",
    "    def update(self):\n",
    "        def loss_function(x, *args) -> float:\n",
    "            compute = x.T @ self.X\n",
    "            compute[compute > 0] = 1\n",
    "            compute[compute <= 0] = -1\n",
    "            condition = compute*self.Y\n",
    "\n",
    "            for i in range(condition.shape[0]):\n",
    "                if condition[i] < -1:\n",
    "                    compute[i] = -4*condition[i]\n",
    "                elif condition[i] > 1:\n",
    "                    compute[i] = 0\n",
    "                else:\n",
    "                    compute[i] = (1 - condition[i])**2\n",
    "            L = np.sum(compute)/condition.shape[0] + self.lamb*(x.T @ x)  \n",
    "            return L\n",
    "        self.u = minimize(loss_function, self.u, method=\"BFGS\", tol=1e-6).x.T\n",
    "\n",
    "a = np.random.rand(1,4)\n",
    "b = np.random.rand(1,4)\n",
    "a = linear_classifier(X_input, Y_input[:,5], \"of\")\n",
    "print(a.u.T)\n",
    "a.update()\n",
    "print(a.u.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = []\n",
    "for i in range(len(prep_dict)):\n",
    "    curr = linear_classifier(X_input, Y_input[:, i], prep[i])\n",
    "    curr.update()\n",
    "    classifier.append(curr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'They usually lack language and can not open a debate'"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test(sentence):\n",
    "    _input = nlp(sentence) \n",
    "    fix = {}\n",
    "    chunk = get_pps(_input, type_list[1], prep_dict)\n",
    "    for i in chunk:\n",
    "        result = []\n",
    "        location = i[1][0]\n",
    "        X_input = input_feature_transform(i[2])\n",
    "        for j in classifier:\n",
    "            predict_val = j.predict(X_input)\n",
    "            result.append((predict_val, j.name))\n",
    "        sorted_result = sorted(result, key=lambda x: x[0],reverse=True)  \n",
    "        if sorted_result[0][0] - sorted_result[1][0] > 0.7:\n",
    "            if sorted_result[0][0] < 0:\n",
    "                fix[location] = \"~\"\n",
    "            else:\n",
    "                fix[location] = sorted_result[0][1]\n",
    "    correct = []         \n",
    "    for i in range(len(_input)):\n",
    "        if i not in fix:\n",
    "            correct.append(_input[i].text)\n",
    "        else:\n",
    "            correct.append(fix[i])\n",
    "    return ' '.join(correct)\n",
    "        \n",
    "test(\"They usually lack language and can not open a debate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv read\n",
      "csv write\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "# Function to read the first column (incorrect sentences) from a CSV\n",
    "def read_incorrect_sentences_from_csv(file_path):\n",
    "    sentences = []\n",
    "    with open(file_path, mode='r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)  # Skip header row if there is one\n",
    "        for row in reader:\n",
    "            sentences.append((row[1], row[0]))  # Assuming the first column has the correct sentences and the second has the incorrect sentences\n",
    "    return sentences\n",
    "\n",
    "# Function to write corrected sentences to a CSV\n",
    "def write_corrected_sentences_to_csv(file_path, sentences):\n",
    "    with open(file_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Target Sentence', 'Corrected Sentence'])  # Header row\n",
    "        for sentence in sentences:\n",
    "            writer.writerow(sentence)\n",
    "\n",
    "# Main process\n",
    "input_csv_path = 'ABC_train_gold_bea19_300samples_copy.csv'  # Path to your input CSV file\n",
    "output_csv_path = 'target.csv'  # Path to your output CSV file\n",
    "\n",
    "sentences = read_incorrect_sentences_from_csv(input_csv_path)\n",
    "print(\"csv read\")\n",
    "corrected_sentences = []\n",
    "for incorrect_sentence, target_sentence in sentences:\n",
    "    corrected = test(incorrect_sentence)\n",
    "    corrected_sentences.append((target_sentence, corrected))\n",
    "\n",
    "write_corrected_sentences_to_csv(output_csv_path, corrected_sentences)\n",
    "print(\"csv write\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-Levenshtein\n",
      "  Downloading python_Levenshtein-0.25.0-py3-none-any.whl (9.4 kB)\n",
      "Collecting Levenshtein==0.25.0\n",
      "  Downloading Levenshtein-0.25.0-cp39-cp39-win_amd64.whl (98 kB)\n",
      "Collecting rapidfuzz<4.0.0,>=3.1.0\n",
      "  Downloading rapidfuzz-3.6.2-cp39-cp39-win_amd64.whl (1.6 MB)\n",
      "Installing collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n",
      "Successfully installed Levenshtein-0.25.0 python-Levenshtein-0.25.0 rapidfuzz-3.6.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jiwer\n",
      "  Downloading jiwer-3.0.3-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: rapidfuzz<4,>=3 in c:\\users\\dell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jiwer) (3.6.2)\n",
      "Collecting click<9.0.0,>=8.1.3\n",
      "  Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from click<9.0.0,>=8.1.3->jiwer) (0.4.6)\n",
      "Installing collected packages: click, jiwer\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 8.0.3\n",
      "    Uninstalling click-8.0.3:\n",
      "      Successfully uninstalled click-8.0.3\n",
      "Successfully installed click-8.1.7 jiwer-3.0.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge\n",
      "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: six in c:\\users\\dell\\appdata\\roaming\\python\\python39\\site-packages (from rouge) (1.15.0)\n",
      "Installing collected packages: rouge\n",
      "Successfully installed rouge-1.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-Levenshtein\n",
    "%pip install jiwer\n",
    "%pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match Accuracy: 0.37606837606837606\n",
      "Average Levenshtein Distance: 5.726495726495727\n",
      "Average Word Error Rate: 0.0924992397749662\n",
      "ROUGE Scores: {'rouge-1': {'r': 0.9406156471258732, 'p': 0.9335302402287936, 'f': 0.9361704547597999}, 'rouge-2': {'r': 0.8532181727904609, 'p': 0.843944912502821, 'f': 0.8479967324143409}, 'rouge-l': {'r': 0.9384486079823948, 'p': 0.9313521698220254, 'f': 0.9339985608652032}}\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "from jiwer import wer\n",
    "from rouge import Rouge\n",
    "\n",
    "# Function to read sentences from a CSV file\n",
    "def read_sentences_from_csv(file_path):\n",
    "    with open(file_path, newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        target_sentences = []\n",
    "        correct_sentences = []\n",
    "        for row in reader:\n",
    "            target_sentences.append(row['Target Sentence'])\n",
    "            correct_sentences.append(row['Corrected Sentence'])\n",
    "    return target_sentences, correct_sentences\n",
    "\n",
    "# Compute Exact Match Accuracy\n",
    "def exact_match_accuracy(targets, corrects):\n",
    "    exact_matches = [1 for target, correct in zip(targets, corrects) if target == correct]\n",
    "    return sum(exact_matches) / len(targets)\n",
    "\n",
    "# Calculate Levenshtein Distance\n",
    "def average_levenshtein_distance(targets, corrects):\n",
    "    distances = [levenshtein_distance(target, correct) for target, correct in zip(targets, corrects)]\n",
    "    return np.mean(distances)\n",
    "\n",
    "# Determine Word Error Rate\n",
    "def average_wer(targets, corrects):\n",
    "    error_rates = [wer(correct, target) for target, correct in zip(targets, corrects)]\n",
    "    return np.mean(error_rates)\n",
    "\n",
    "# Compute ROUGE Score\n",
    "def compute_rouge(targets, corrects):\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(targets, corrects, avg=True)\n",
    "    return scores\n",
    "\n",
    "# Main function to compute metrics\n",
    "def main(csv_file_path):\n",
    "    target_sentences, correct_sentences = read_sentences_from_csv(csv_file_path)\n",
    "    \n",
    "    exact_match = exact_match_accuracy(target_sentences, correct_sentences)\n",
    "    levenshtein_dist = average_levenshtein_distance(target_sentences, correct_sentences)\n",
    "    word_error_rate = average_wer(target_sentences, correct_sentences)\n",
    "    rouge_scores = compute_rouge(target_sentences, correct_sentences)\n",
    "    \n",
    "    print(f\"Exact Match Accuracy: {exact_match}\")\n",
    "    print(f\"Average Levenshtein Distance: {levenshtein_dist}\")\n",
    "    print(f\"Average Word Error Rate: {word_error_rate}\")\n",
    "    print(\"ROUGE Scores:\", rouge_scores)\n",
    "\n",
    "# Replace 'sentences.csv' with the path to your actual CSV file\n",
    "main('target.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
