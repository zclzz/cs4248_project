{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33db098f-73cf-4f8e-9713-249b74d08e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-04-09 14:00:03.952\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mDataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 33935\n",
      "})\u001b[0m\n",
      "\u001b[32m2024-04-09 14:00:03.953\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mDataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 4384\n",
      "})\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import os\n",
    "import argparse\n",
    "from transformers import AutoTokenizer, BartForConditionalGeneration, Text2TextGenerationPipeline\n",
    "from transformers import HfArgumentParser, TrainingArguments, Trainer, set_seed\n",
    "from datasets import load_dataset, Dataset\n",
    "from loguru import logger\n",
    "import torch\n",
    "import csv\n",
    "\n",
    "class CsvDataset(object):\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "\n",
    "    def load(self):\n",
    "        data_list = []\n",
    "        with open(self.file_path, 'r', encoding='utf-8') as file:\n",
    "            reader = csv.reader(file)\n",
    "            # Skip the first row \n",
    "            next(reader)\n",
    "            for row in reader:\n",
    "                # correct, incorrect\n",
    "                correct_text = row[0]\n",
    "                incorrect_text = row[1]\n",
    "                data_list.append(correct_text + '\\t' + incorrect_text)\n",
    "        return {'text': data_list}\n",
    "\n",
    "d = CsvDataset(\"./data/data/train_cleaned.csv\")\n",
    "data_dict = d.load()\n",
    "train_dataset = Dataset.from_dict(data_dict, split='train')\n",
    "\n",
    "d = CsvDataset(\"./data/data/dev.csv\")\n",
    "data_dict = d.load()\n",
    "valid_dataset = Dataset.from_dict(data_dict, split='test')\n",
    "logger.info(train_dataset)\n",
    "logger.info(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a51945d-d563-4e73-8141-2d7f484608c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/environment/miniconda3/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bebde00a-c37d-4f56-a40e-66c22029000b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 33935/33935 [00:11<00:00, 3044.79 examples/s]\n",
      "Map: 100%|██████████| 4384/4384 [00:01<00:00, 2659.05 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_dataset(tokenizer, dataset, max_len):\n",
    "    def convert_to_features(example_batch):\n",
    "        src_texts = []\n",
    "        trg_texts = []\n",
    "        for example in example_batch['text']:\n",
    "            terms = example.split('\\t', 1)\n",
    "            src_texts.append(terms[0])\n",
    "            trg_texts.append(terms[1])\n",
    "        input_encodings = tokenizer.batch_encode_plus(\n",
    "            src_texts,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=max_len,\n",
    "        )\n",
    "        target_encodings = tokenizer.batch_encode_plus(\n",
    "            trg_texts,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=max_len,\n",
    "        )\n",
    "\n",
    "        encodings = {\n",
    "            'input_ids': input_encodings['input_ids'],\n",
    "            'attention_mask': input_encodings['attention_mask'],\n",
    "            'target_ids': target_encodings['input_ids'],\n",
    "            'target_attention_mask': target_encodings['attention_mask']\n",
    "        }\n",
    "\n",
    "        return encodings\n",
    "    dataset = dataset.map(convert_to_features, batched=True)\n",
    "    # Set the tensor type and the columns which the dataset should return\n",
    "    columns = ['input_ids', 'target_ids', 'attention_mask', 'target_attention_mask']\n",
    "    dataset.with_format(type='torch', columns=columns)\n",
    "    # Rename columns to the names that the forward method of the selected\n",
    "    # model expects\n",
    "    dataset = dataset.rename_column('target_ids', 'labels')\n",
    "    dataset = dataset.rename_column('target_attention_mask', 'decoder_attention_mask')\n",
    "    dataset = dataset.remove_columns(['text'])\n",
    "    return dataset\n",
    "\n",
    "train_data = tokenize_dataset(tokenizer, train_dataset,128)\n",
    "valid_data = tokenize_dataset(tokenizer, valid_dataset,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87ed818b-9ce7-446d-b5f0-72d138850caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/environment/miniconda3/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5305' max='5305' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5305/5305 1:01:58, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.569200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.203700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.152900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.247200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.177400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.166300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.157500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.123300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.125700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.117300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.104600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.107200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.106400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.101900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.097600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.101000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.098200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.100800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.099100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.094700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.090300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.085200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.093000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.083800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.084600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.085200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.086500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.085500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.088400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.084200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.086600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.082700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.081400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.084200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.101800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.101500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.084800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.083000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.081700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.088800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.084800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.076000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.070200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.070100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.069300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.072200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.069200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.070900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.070700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.073200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.069600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.072000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.066500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.072200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.065400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.064500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.067500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.067200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.063200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.070000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.071100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.072900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.063000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.052600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.052700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.053700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.055500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.053900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.055500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.054400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.053600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>0.054300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.052800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.054800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.054500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>0.050600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.053400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>0.051800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.053400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>0.050600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.051300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>0.053100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.053800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.042100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>0.041200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.041500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>0.041600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.038900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>0.038500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.040400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>0.040400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.042000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>0.039700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.038000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>0.040100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.040700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>0.040200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.039400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>0.039700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.041400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>0.042300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.040800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>0.039700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.039000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    num_train_epochs=5,          # total # of training epochs \n",
    "    per_device_train_batch_size=32,  # batch size per device during training \n",
    "    per_device_eval_batch_size=32,   # batch size for evaluation \n",
    "    learning_rate=1e-4,             # learning rate\n",
    "    save_steps=False,\n",
    "    logging_steps=50\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=valid_data,\n",
    ")\n",
    "trainer.train()\n",
    "## save models\n",
    "model.save_pretrained(\"result_bart/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f69f0558-19d8-4885-b9fa-d747b7984201",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = BartForConditionalGeneration.from_pretrained(\"./result_bart/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04732090-b40d-41fd-8dd8-25990fe85262",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "new_model = BartForConditionalGeneration.from_pretrained(\"./bart_model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5423dbab-c3ce-4a13-a1d3-51cb87c2a647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bart_correct(tokenizer, model, text: str, max_length: int = 128):\n",
    "    import numpy as np\n",
    "\n",
    "    text = \"< \" + text\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    \n",
    "    input_ids = inputs['input_ids'][:, :max_length]\n",
    "    attention_mask = inputs['attention_mask'][:, :max_length]\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, attention_mask=attention_mask).logits\n",
    "        \n",
    "        predicted_token_indexes = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        decode_tokens = tokenizer.decode(predicted_token_indexes[0], skip_special_tokens=True)\n",
    "        \n",
    "        decode_tokens = decode_tokens.strip()\n",
    "        \n",
    "    return decode_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8be3b5e-4c36-4e0d-9650-226cff3c74f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He is a nice person.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_correct(tokenizer, new_model, \"He are a nice person.\", 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e218fd90-f4b9-4d6e-8165-ce0ad6ca8d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test = pd.read_csv('data/train_300samples.csv',na_filter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89614254-a890-41ea-aaea-d61ec4f43141",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def pred_sentences(df):\n",
    "    temp = pd.DataFrame()\n",
    "    a = []\n",
    "    b = []\n",
    "    for i in tqdm(range(300)):\n",
    "        #if i == 170:\n",
    "        #    continue\n",
    "        reference = df['correct'].iloc[i]\n",
    "        pred = bart_correct(tokenizer, new_model, df['incorrect'].iloc[i], 128)\n",
    "        tokens = word_tokenize(pred)\n",
    "        pred = \" \".join(tokens)\n",
    "        a.append(reference)\n",
    "        b.append(pred)\n",
    "    temp['Target Sentence'] = a\n",
    "    temp['Corrected Sentence'] = b\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08cfd92b-8b1f-437a-82f8-1d68c6f714a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 300/300 [03:25<00:00,  1.46it/s]\n"
     ]
    }
   ],
   "source": [
    "temp = pd.DataFrame()\n",
    "temp = pred_sentences(test)\n",
    "temp.to_csv('data/sentences.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aabe23c3-2e18-43ff-b6c1-090cac8fa568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match Accuracy: 0.30333333333333334\n",
      "Average Levenshtein Distance: 7.8966666666666665\n",
      "Average Word Error Rate: 0.11998602052400152\n",
      "ROUGE Scores: {'rouge-1': {'r': 0.9209161400211485, 'p': 0.9128017739721874, 'f': 0.9157767270105097}, 'rouge-2': {'r': 0.8111905453400288, 'p': 0.8044661676197792, 'f': 0.8069622536383714}, 'rouge-l': {'r': 0.9168034109452109, 'p': 0.9086315376565004, 'f': 0.9116454336142031}}\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "from jiwer import wer\n",
    "from rouge import Rouge\n",
    "\n",
    "# Function to read sentences from a CSV file\n",
    "def read_sentences_from_csv(file_path):\n",
    "    with open(file_path, newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        target_sentences = []\n",
    "        correct_sentences = []\n",
    "        for row in reader:\n",
    "            target_sentences.append(row['Target Sentence'])\n",
    "            correct_sentences.append(row['Corrected Sentence'])\n",
    "    return target_sentences, correct_sentences\n",
    "\n",
    "# Compute Exact Match Accuracy\n",
    "def exact_match_accuracy(targets, corrects):\n",
    "    exact_matches = [1 for target, correct in zip(targets, corrects) if target == correct]\n",
    "    return sum(exact_matches) / len(targets)\n",
    "\n",
    "# Calculate Levenshtein Distance\n",
    "def average_levenshtein_distance(targets, corrects):\n",
    "    distances = [levenshtein_distance(target, correct) for target, correct in zip(targets, corrects)]\n",
    "    return np.mean(distances)\n",
    "\n",
    "# Determine Word Error Rate\n",
    "def average_wer(targets, corrects):\n",
    "    error_rates = [wer(correct, target) for target, correct in zip(targets, corrects)]\n",
    "    return np.mean(error_rates)\n",
    "\n",
    "# Compute ROUGE Score\n",
    "def compute_rouge(targets, corrects):\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(targets, corrects, avg=True)\n",
    "    return scores\n",
    "\n",
    "# Main function to compute metrics\n",
    "def main(csv_file_path):\n",
    "    target_sentences, correct_sentences = read_sentences_from_csv(csv_file_path)\n",
    "    \n",
    "    exact_match = exact_match_accuracy(target_sentences, correct_sentences)\n",
    "    levenshtein_dist = average_levenshtein_distance(target_sentences, correct_sentences)\n",
    "    word_error_rate = average_wer(target_sentences, correct_sentences)\n",
    "    rouge_scores = compute_rouge(target_sentences, correct_sentences)\n",
    "    \n",
    "    print(f\"Exact Match Accuracy: {exact_match}\")\n",
    "    print(f\"Average Levenshtein Distance: {levenshtein_dist}\")\n",
    "    print(f\"Average Word Error Rate: {word_error_rate}\")\n",
    "    print(\"ROUGE Scores:\", rouge_scores)\n",
    "\n",
    "# Replace 'sentences.csv' with the path to your actual CSV file\n",
    "main('data/sentences.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11766b4-00c1-4d02-8ef7-058c64f41933",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
